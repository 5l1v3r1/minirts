{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimalist Tower RTS Agent\n",
    "Sam Greydanus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, time, os, glob\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from towers import Towers\n",
    "from rtsenv import RTSEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation generally follows the documentation of Zoe's version. Changes: tower locations, within quadrants, are randomized. Tower values (\"healths\" aka \"hit points\"), within quadrants, are randomized. Agent value is randomized. Final reward is \"tower value - agent value IF agent value is greater ELSE reward is -3\". Friend towers are treated the same as enemy towers in the reward computation, except their magnitudes are negative.\n",
    "\n",
    "Channel Overview\n",
    " * channel 1 - hit point channel **NON BINARY**\n",
    " * channel 2 - agent mask\n",
    " * channel 3 - small tower mask\n",
    " * channel 4 - large tower mask\n",
    " * channel 5 - friendly mask\n",
    " * channel 6 - enemy mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(torch.nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(6, 16, 2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 2, stride=2, padding=1)\n",
    "        self.flat_dim = flat_dim = 16 * 3 * 3\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(flat_dim, 1), nn.Linear(flat_dim, num_actions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        hx = x.view(-1, self.flat_dim)\n",
    "        value, probs = self.critic_linear(hx), F.softmax(self.actor_linear(hx), dim=1)\n",
    "        return value, probs\n",
    "    \n",
    "    def try_load(self, save_dir):\n",
    "        paths = glob.glob(save_dir + '*.tar') ; rew = None\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [float(s.split('_')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; rew = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if rew is None else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.lr = 1e-3\n",
    "args.num_actions = 4\n",
    "args.batch_size = 128\n",
    "args.total_steps = 20000 # just set this to a big number\n",
    "args.printevery = 5 # print stats every n seconds\n",
    "args.saveevery = 30 # save model every n seconds\n",
    "args.rewardthresh = -0.3\n",
    "args.save_dir = './saved/'\n",
    "\n",
    "os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None # make dir to save models etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the environment, train an agent\n",
    "\n",
    "PyTorch has a great API for reinforcement learning now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training a simple policy gradient CNN agent on the towers RTS game\n",
      "\tstep 139, value loss -0.003, policy loss -0.003, reward -1.079\n",
      "\tstep 293, value loss -0.023, policy loss -0.023, reward -1.062\n",
      "\tstep 428, value loss -0.127, policy loss -0.127, reward -0.634\n",
      "\tstep 569, value loss -0.088, policy loss -0.088, reward -0.430\n",
      "\tstep 695, value loss -0.084, policy loss -0.084, reward -0.385\n",
      "\t\tsaved model_step_830_reward_-0.35_.tar\n",
      "\tstep 832, value loss -0.088, policy loss -0.088, reward -0.354\n",
      "\tstep 974, value loss -0.098, policy loss -0.098, reward -0.347\n",
      "\tstep 1102, value loss -0.086, policy loss -0.086, reward -0.352\n",
      "\tstep 1231, value loss -0.083, policy loss -0.083, reward -0.338\n",
      "\tstep 1379, value loss -0.082, policy loss -0.082, reward -0.329\n",
      "\tstep 1493, value loss -0.093, policy loss -0.093, reward -0.334\n",
      "\t\tsaved model_step_1587_reward_-0.34_.tar\n",
      "\tstep 1592, value loss -0.095, policy loss -0.095, reward -0.334\n",
      "\tstep 1712, value loss -0.099, policy loss -0.099, reward -0.332\n",
      "\tstep 1831, value loss -0.084, policy loss -0.084, reward -0.314\n",
      "\tstep 1952, value loss -0.092, policy loss -0.092, reward -0.314\n",
      "\tstep 2072, value loss -0.093, policy loss -0.093, reward -0.311\n",
      "\tstep 2193, value loss -0.097, policy loss -0.097, reward -0.313\n",
      "environment solved\n",
      "\t\tsaved model_step_2242_reward_-0.30_.tar\n"
     ]
    }
   ],
   "source": [
    "model = NNPolicy(num_actions=args.num_actions)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "run_vloss, run_ploss, run_reward = None, None, None\n",
    "\n",
    "print('training a simple policy gradient CNN agent on the towers RTS game')\n",
    "print_t = save_t = time.time()\n",
    "for step in range(args.total_steps + 1):\n",
    "    envs = [Towers() for i in range(args.batch_size)]\n",
    "    obs = np.stack([e.reset() for e in envs])\n",
    "    state = Variable(torch.Tensor(obs))\n",
    "    value, probs = model(state)\n",
    "\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    actions = m.sample()\n",
    "    raw_reward = [e.step(actions.data.numpy().ravel()[i])[1] for i, e in enumerate(envs)]\n",
    "    raw_reward = Variable(torch.Tensor(raw_reward))\n",
    "    norm_reward = (raw_reward - raw_reward.mean())/raw_reward.std() # this sometimes helps convergence\n",
    "\n",
    "    vloss = 0.5*(value.view(-1) - norm_reward).pow(2).mean()\n",
    "    ploss = (-m.log_prob(actions) * norm_reward).mean()\n",
    "    (vloss + ploss).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    np_vloss = vloss.data.view(-1)[0]\n",
    "    np_ploss = ploss.data.view(-1)[0]\n",
    "    np_reward = raw_reward.mean().data.view(-1)[0]\n",
    "\n",
    "    run_vloss = np_vloss if run_vloss is None else 0.99*run_ploss + 0.01*np_ploss\n",
    "    run_ploss = np_ploss if run_ploss is None else 0.99*run_ploss + 0.01*np_ploss\n",
    "    run_reward = np_reward if run_reward is None else 0.99*run_reward + 0.01*np_reward\n",
    "    \n",
    "    if time.time() - print_t > args.printevery:\n",
    "        print_t = time.time()\n",
    "        print('\\tstep {}, value loss {:.3f}, policy loss {:.3f}, reward {:.3f}'\n",
    "              .format(step, run_vloss, run_ploss, run_reward))\n",
    "    \n",
    "    if time.time() - save_t > args.saveevery:\n",
    "        save_t = time.time()\n",
    "        paths = glob.glob(args.save_dir + '*.tar') ; rew_best_saved_model = -10000\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [float(s.split('_')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; rew_best_saved_model = ckpts[ix]\n",
    "        if rew_best_saved_model < run_reward:\n",
    "            name = 'model_step_{:.0f}_reward_{:.2f}_.tar'.format(step,run_reward)\n",
    "            print('\\t\\tsaved {}'.format(name))\n",
    "            torch.save(model.state_dict(), args.save_dir + name)\n",
    "        else:\n",
    "            print('\\t\\tlol your model sucks keep training')\n",
    "        \n",
    "    if run_reward > args.rewardthresh:\n",
    "        print(\"environment solved\")\n",
    "        name = 'model_step_{:.0f}_reward_{:.2f}_.tar'.format(step,run_reward) ; print('\\t\\tsaved {}'.format(name))\n",
    "        torch.save(model.state_dict(), args.save_dir + name)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try loading a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: ./saved/model_step_2242_reward_-0.30_.tar\n",
      "this saved model had a mean reward of -0.30\n"
     ]
    }
   ],
   "source": [
    "model = NNPolicy(num_actions=args.num_actions)\n",
    "ckpt_reward = model.try_load(args.save_dir)\n",
    "print('this saved model had a mean reward of {:.2f}'.format(ckpt_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
