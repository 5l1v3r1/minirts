{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimalist Tower RTS Agent\n",
    "Sam Greydanus\n",
    "\n",
    "See the `minirts-env` notebook in this directory for details on the RTS environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, time, os, glob\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from towers import Towers\n",
    "from rtsenv import RTSEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(torch.nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(6, 16, 2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 2, stride=2, padding=1)\n",
    "        self.flat_dim = flat_dim = 16 * 3 * 3\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(flat_dim, 1), nn.Linear(flat_dim, num_actions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        hx = x.view(-1, self.flat_dim)\n",
    "        value, probs = self.critic_linear(hx), F.softmax(self.actor_linear(hx), dim=1)\n",
    "        return value, probs\n",
    "    \n",
    "    def try_load(self, save_dir):\n",
    "        paths = glob.glob(save_dir + '*.tar') ; rew = None\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [float(s.split('_')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; rew = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if rew is None else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.lr = 1e-3\n",
    "args.num_actions = 4\n",
    "args.batch_size = 128\n",
    "args.total_steps = 20000 # just set this to a big number\n",
    "args.printevery = 5 # print stats every n seconds\n",
    "args.saveevery = 30 # save model every n seconds\n",
    "args.rewardthresh = -0.3\n",
    "args.save_dir = './saved/'\n",
    "\n",
    "os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None # make dir to save models etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the environment, train an agent\n",
    "\n",
    "PyTorch has a great API for reinforcement learning now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training a simple policy gradient CNN agent on the towers RTS game\n",
      "\tstep 164, value loss 0.450, policy loss 0.000, reward -1.003\n",
      "\tstep 329, value loss 0.425, policy loss -0.027, reward -0.974\n",
      "\tstep 482, value loss 0.422, policy loss -0.136, reward -0.867\n",
      "\tstep 640, value loss 0.421, policy loss -0.175, reward -0.709\n",
      "\tstep 807, value loss 0.411, policy loss -0.127, reward -0.555\n",
      "\t\tsaved model_step_959_reward_-0.51_.tar\n",
      "\tstep 962, value loss 0.402, policy loss -0.110, reward -0.512\n",
      "\tstep 1108, value loss 0.397, policy loss -0.109, reward -0.485\n",
      "\tstep 1238, value loss 0.394, policy loss -0.105, reward -0.458\n",
      "\tstep 1358, value loss 0.397, policy loss -0.111, reward -0.445\n",
      "\tstep 1472, value loss 0.395, policy loss -0.119, reward -0.433\n",
      "\tstep 1614, value loss 0.391, policy loss -0.123, reward -0.406\n",
      "\t\tsaved model_step_1748_reward_-0.39_.tar\n",
      "\tstep 1753, value loss 0.382, policy loss -0.117, reward -0.392\n",
      "\tstep 1885, value loss 0.376, policy loss -0.121, reward -0.378\n",
      "\tstep 2016, value loss 0.369, policy loss -0.127, reward -0.352\n",
      "\tstep 2150, value loss 0.367, policy loss -0.139, reward -0.348\n",
      "\tstep 2270, value loss 0.366, policy loss -0.125, reward -0.328\n",
      "\tstep 2385, value loss 0.366, policy loss -0.143, reward -0.313\n",
      "\t\tsaved model_step_2504_reward_-0.30_.tar\n",
      "\tstep 2513, value loss 0.358, policy loss -0.137, reward -0.307\n",
      "\tstep 2664, value loss 0.359, policy loss -0.149, reward -0.303\n",
      "environment solved\n",
      "\t\tsaved model_step_2724_reward_-0.30_.tar\n"
     ]
    }
   ],
   "source": [
    "model = NNPolicy(num_actions=args.num_actions)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "run_vloss, run_ploss, run_reward = None, None, None\n",
    "\n",
    "print('training a simple policy gradient CNN agent on the towers RTS game')\n",
    "print_t = save_t = time.time()\n",
    "for step in range(args.total_steps + 1):\n",
    "    envs = [Towers() for i in range(args.batch_size)]\n",
    "    obs = np.stack([e.reset() for e in envs])\n",
    "    state = Variable(torch.Tensor(obs))\n",
    "    value, probs = model(state)\n",
    "\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    actions = m.sample()\n",
    "    raw_reward = [e.step(actions.data.numpy().ravel()[i])[1] for i, e in enumerate(envs)]\n",
    "    raw_reward = Variable(torch.Tensor(raw_reward))\n",
    "    norm_reward = (raw_reward - raw_reward.mean())/raw_reward.std() # this sometimes helps convergence\n",
    "\n",
    "    vloss = 0.5*(value.view(-1) - norm_reward).pow(2).mean()\n",
    "    ploss = (-m.log_prob(actions) * norm_reward).mean()\n",
    "    (vloss + ploss).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    np_vloss = vloss.data.view(-1)[0]\n",
    "    np_ploss = ploss.data.view(-1)[0]\n",
    "    np_reward = raw_reward.mean().data.view(-1)[0]\n",
    "\n",
    "    run_vloss = np_vloss if run_vloss is None else 0.99*run_vloss + 0.01*np_vloss\n",
    "    run_ploss = np_ploss if run_ploss is None else 0.99*run_ploss + 0.01*np_ploss\n",
    "    run_reward = np_reward if run_reward is None else 0.99*run_reward + 0.01*np_reward\n",
    "    \n",
    "    if time.time() - print_t > args.printevery:\n",
    "        print_t = time.time()\n",
    "        print('\\tstep {}, value loss {:.3f}, policy loss {:.3f}, reward {:.3f}'\n",
    "              .format(step, run_vloss, run_ploss, run_reward))\n",
    "    \n",
    "    if time.time() - save_t > args.saveevery:\n",
    "        save_t = time.time()\n",
    "        paths = glob.glob(args.save_dir + '*.tar') ; rew_best_saved_model = -10000\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [float(s.split('_')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; rew_best_saved_model = ckpts[ix]\n",
    "        if rew_best_saved_model < run_reward:\n",
    "            name = 'model_step_{:.0f}_reward_{:.2f}_.tar'.format(step,run_reward)\n",
    "            print('\\t\\tsaved {}'.format(name))\n",
    "            torch.save(model.state_dict(), args.save_dir + name)\n",
    "        else:\n",
    "            print('\\t\\tlol your model sucks keep training')\n",
    "        \n",
    "    if run_reward > args.rewardthresh:\n",
    "        print(\"environment solved\")\n",
    "        name = 'model_step_{:.0f}_reward_{:.2f}_.tar'.format(step,run_reward) ; print('\\t\\tsaved {}'.format(name))\n",
    "        torch.save(model.state_dict(), args.save_dir + name)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try loading a saved model\n",
    "\n",
    "Yeah, the saving and loading I/O works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: ./saved/model_step_2504_reward_-0.30_.tar\n",
      "this saved model had a mean reward of -0.30\n"
     ]
    }
   ],
   "source": [
    "model = NNPolicy(num_actions=args.num_actions)\n",
    "ckpt_reward = model.try_load(args.save_dir)\n",
    "print('this saved model had a mean reward of {:.2f}'.format(ckpt_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
