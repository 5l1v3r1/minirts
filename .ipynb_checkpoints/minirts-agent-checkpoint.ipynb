{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimalist Tower RTS Agent\n",
    "Sam Greydanus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, time, os, glob\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from towers import Towers\n",
    "from rtsenv import RTSEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation generally follows the documentation of Zoe's version. Changes: tower locations, within quadrants, are randomized. Tower values (\"healths\" aka \"hit points\"), within quadrants, are randomized. Agent value is randomized. Final reward is \"tower value - agent value IF agent value is greater ELSE reward is -3\". Friend towers are treated the same as enemy towers in the reward computation, except their magnitudes are negative.\n",
    "\n",
    "Channel Overview\n",
    " * channel 1 - hit point channel **NON BINARY**\n",
    " * channel 2 - agent mask\n",
    " * channel 3 - small tower mask\n",
    " * channel 4 - large tower mask\n",
    " * channel 5 - friendly mask\n",
    " * channel 6 - enemy mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(torch.nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(6, 16, 2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 2, stride=2, padding=1)\n",
    "        self.flat_dim = flat_dim = 16 * 3 * 3\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(flat_dim, 1), nn.Linear(flat_dim, num_actions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        hx = x.view(-1, self.flat_dim)\n",
    "        value, probs = self.critic_linear(hx), F.softmax(self.actor_linear(hx), dim=1)\n",
    "        return value, probs\n",
    "    \n",
    "    def try_load(self, save_dir):\n",
    "        paths = glob.glob(save_dir + '*.tar') ; rew = None\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [float(s.split('_')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; rew = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if rew is None else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.lr = 1e-3\n",
    "args.num_actions = 4\n",
    "args.batch_size = 128\n",
    "args.total_steps = 20000 # just set this to a big number\n",
    "args.printevery = 5 # print stats every n seconds\n",
    "args.saveevery = 30 # save model every n seconds\n",
    "args.rewardthresh = -0.3\n",
    "args.save_dir = './saved/'\n",
    "\n",
    "os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None # make dir to save models etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the environment, train an agent\n",
    "\n",
    "PyTorch has a great API for reinforcement learning now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training a simple policy gradient CNN agent on the towers RTS game\n",
      "\tstep 144, value loss -1.475, policy loss -1.475, reward -1.104\n",
      "\tstep 298, value loss -1.447, policy loss -1.447, reward -1.092\n",
      "\tstep 454, value loss -1.172, policy loss -1.172, reward -0.848\n",
      "\tstep 609, value loss -0.576, policy loss -0.576, reward -0.505\n",
      "\tstep 750, value loss -0.349, policy loss -0.349, reward -0.406\n",
      "\t\tlol your model sucks keep training\n",
      "\tstep 860, value loss -0.275, policy loss -0.275, reward -0.373\n",
      "\tstep 1002, value loss -0.232, policy loss -0.232, reward -0.357\n",
      "\tstep 1146, value loss -0.216, policy loss -0.216, reward -0.363\n",
      "\tstep 1296, value loss -0.208, policy loss -0.208, reward -0.353\n",
      "\tstep 1454, value loss -0.196, policy loss -0.196, reward -0.349\n",
      "\tstep 1600, value loss -0.191, policy loss -0.191, reward -0.340\n",
      "\t\tlol your model sucks keep training\n",
      "\tstep 1752, value loss -0.189, policy loss -0.189, reward -0.338\n",
      "\tstep 1900, value loss -0.185, policy loss -0.185, reward -0.341\n",
      "\tstep 2053, value loss -0.183, policy loss -0.183, reward -0.346\n",
      "\tstep 2207, value loss -0.174, policy loss -0.174, reward -0.329\n",
      "\tstep 2358, value loss -0.176, policy loss -0.176, reward -0.325\n",
      "\tstep 2503, value loss -0.169, policy loss -0.169, reward -0.326\n",
      "\t\tlol your model sucks keep training\n",
      "\tstep 2648, value loss -0.170, policy loss -0.170, reward -0.321\n",
      "\tstep 2803, value loss -0.163, policy loss -0.163, reward -0.315\n",
      "\tstep 2941, value loss -0.159, policy loss -0.159, reward -0.320\n",
      "\tstep 3033, value loss -0.161, policy loss -0.161, reward -0.311\n",
      "\tstep 3151, value loss -0.161, policy loss -0.161, reward -0.310\n",
      "\tstep 3280, value loss -0.153, policy loss -0.153, reward -0.311\n",
      "environment solved\n",
      "\t\tsaved model_step_3341_reward_-0.30_.tar\n"
     ]
    }
   ],
   "source": [
    "model = NNPolicy(num_actions=args.num_actions)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "run_vloss, run_ploss, run_reward = None, None, None\n",
    "\n",
    "print('training a simple policy gradient CNN agent on the towers RTS game')\n",
    "print_t = save_t = time.time()\n",
    "for step in range(args.total_steps + 1):\n",
    "    envs = [Towers() for i in range(args.batch_size)]\n",
    "    obs = np.stack([e.reset() for e in envs])\n",
    "    state = Variable(torch.Tensor(obs))\n",
    "    value, probs = model(state)\n",
    "\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    actions = m.sample()\n",
    "    raw_reward = [e.step(actions.data.numpy().ravel()[i])[1] for i, e in enumerate(envs)]\n",
    "    raw_reward = Variable(torch.Tensor(raw_reward))\n",
    "    norm_reward = (raw_reward - raw_reward.mean())/raw_reward.std() # this sometimes helps convergence\n",
    "\n",
    "    vloss = 0.5*(value.view(-1) - norm_reward).pow(2).mean()\n",
    "    ploss = (-m.log_prob(actions) * norm_reward).mean()\n",
    "    (vloss + ploss).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    np_vloss = vloss.data.view(-1)[0]\n",
    "    np_ploss = ploss.data.view(-1)[0]\n",
    "    np_reward = raw_reward.mean().data.view(-1)[0]\n",
    "\n",
    "    run_vloss = np_vloss if run_vloss is None else 0.99*run_ploss + 0.01*np_ploss\n",
    "    run_ploss = np_ploss if run_ploss is None else 0.99*run_ploss + 0.01*np_ploss\n",
    "    run_reward = np_reward if run_reward is None else 0.99*run_reward + 0.01*np_reward\n",
    "    \n",
    "    if time.time() - print_t > args.printevery:\n",
    "        print_t = time.time()\n",
    "        print('\\tstep {}, value loss {:.3f}, policy loss {:.3f}, reward {:.3f}'\n",
    "              .format(step, run_vloss, run_ploss, run_reward))\n",
    "    \n",
    "    if time.time() - save_t > args.saveevery:\n",
    "        save_t = time.time()\n",
    "        paths = glob.glob(args.save_dir + '*.tar') ; rew_best_saved_model = -10000\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [float(s.split('_')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; rew_best_saved_model = ckpts[ix]\n",
    "        if rew_best_saved_model < run_reward:\n",
    "            name = 'model_step_{:.0f}_reward_{:.2f}_.tar'.format(step,run_reward)\n",
    "            print('\\t\\tsaved {}'.format(name))\n",
    "            torch.save(model.state_dict(), args.save_dir + name)\n",
    "        else:\n",
    "            print('\\t\\tlol your model sucks keep training')\n",
    "        \n",
    "    if run_reward > args.rewardthresh:\n",
    "        print(\"environment solved\")\n",
    "        name = 'model_step_{:.0f}_reward_{:.2f}_.tar'.format(step,run_reward) ; print('\\t\\tsaved {}'.format(name))\n",
    "        torch.save(model.state_dict(), args.save_dir + name)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try loading a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: ./saved/model_step_3341_reward_-0.30_.tar\n",
      "this saved model had a mean reward of -0.30\n"
     ]
    }
   ],
   "source": [
    "model = NNPolicy(num_actions=args.num_actions)\n",
    "ckpt_reward = model.try_load(args.save_dir)\n",
    "print('this saved model had a mean reward of {:.2f}'.format(ckpt_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
