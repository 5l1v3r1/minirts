{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimalist Tower RTS Agent\n",
    "Sam Greydanus\n",
    "\n",
    "See the `minirts-env` notebook in this directory for details on the RTS environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, time, os, glob\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from towers import Towers\n",
    "from rtsenv import RTSEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(torch.nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(6, 16, 2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 2, stride=2, padding=1)\n",
    "        self.flat_dim = flat_dim = 16 * 3 * 3\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(flat_dim, 1), nn.Linear(flat_dim, num_actions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        hx = x.view(-1, self.flat_dim)\n",
    "        value, probs = self.critic_linear(hx), F.softmax(self.actor_linear(hx), dim=1)\n",
    "        return value, probs\n",
    "    \n",
    "    def try_load(self, save_dir):\n",
    "        paths = glob.glob(save_dir + '*.tar') ; rew = None\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [float(s.split('_')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; rew = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if rew is None else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.lr = 1e-3\n",
    "args.num_actions = 4\n",
    "args.batch_size = 128\n",
    "args.total_steps = 20000 # just set this to a big number\n",
    "args.printevery = 5 # print stats every n seconds\n",
    "args.saveevery = 30 # save model every n seconds\n",
    "args.rewardthresh = -0.05\n",
    "args.noiselevel = 0.05\n",
    "args.save_dir = './saved/'\n",
    "\n",
    "os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None # make dir to save models etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the environment, train an agent\n",
    "\n",
    "PyTorch has a great API for reinforcement learning now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training a simple policy gradient CNN agent on the towers RTS game\n",
      "\tstep 117, value loss 0.435, policy loss -0.019, reward -0.331\n",
      "\tstep 240, value loss 0.393, policy loss -0.028, reward -0.265\n",
      "\tstep 353, value loss 0.382, policy loss -0.038, reward -0.191\n",
      "\tstep 456, value loss 0.368, policy loss -0.041, reward -0.157\n",
      "\tstep 562, value loss 0.359, policy loss -0.039, reward -0.143\n",
      "\t\tsaved model_step_658_reward_-0.138_.tar\n",
      "\tstep 662, value loss 0.350, policy loss -0.031, reward -0.138\n",
      "\tstep 758, value loss 0.351, policy loss -0.027, reward -0.131\n",
      "\tstep 843, value loss 0.347, policy loss -0.018, reward -0.131\n",
      "\tstep 951, value loss 0.342, policy loss -0.022, reward -0.128\n",
      "\tstep 1061, value loss 0.353, policy loss -0.026, reward -0.122\n",
      "\tstep 1162, value loss 0.351, policy loss -0.023, reward -0.119\n",
      "\t\tsaved model_step_1249_reward_-0.117_.tar\n",
      "\tstep 1254, value loss 0.359, policy loss -0.026, reward -0.118\n",
      "\tstep 1359, value loss 0.362, policy loss -0.029, reward -0.113\n",
      "\tstep 1455, value loss 0.361, policy loss -0.027, reward -0.107\n",
      "\tstep 1551, value loss 0.366, policy loss -0.026, reward -0.099\n",
      "\tstep 1662, value loss 0.366, policy loss -0.029, reward -0.091\n",
      "\tstep 1773, value loss 0.363, policy loss -0.032, reward -0.090\n",
      "\t\tsaved model_step_1882_reward_-0.083_.tar\n",
      "\tstep 1891, value loss 0.365, policy loss -0.029, reward -0.084\n",
      "\tstep 2003, value loss 0.366, policy loss -0.024, reward -0.079\n",
      "\tstep 2096, value loss 0.363, policy loss -0.021, reward -0.079\n",
      "\tstep 2191, value loss 0.371, policy loss -0.020, reward -0.082\n",
      "\tstep 2287, value loss 0.371, policy loss -0.023, reward -0.081\n",
      "\tstep 2392, value loss 0.365, policy loss -0.026, reward -0.081\n",
      "\t\tsaved model_step_2500_reward_-0.078_.tar\n",
      "\tstep 2511, value loss 0.367, policy loss -0.017, reward -0.077\n",
      "\tstep 2611, value loss 0.369, policy loss -0.020, reward -0.085\n",
      "\tstep 2705, value loss 0.369, policy loss -0.017, reward -0.083\n",
      "\tstep 2785, value loss 0.361, policy loss -0.019, reward -0.082\n",
      "\tstep 2885, value loss 0.360, policy loss -0.019, reward -0.081\n",
      "\tstep 2994, value loss 0.363, policy loss -0.016, reward -0.080\n",
      "\t\tsaved model_step_3101_reward_-0.078_.tar\n",
      "\tstep 3115, value loss 0.371, policy loss -0.017, reward -0.078\n",
      "\tstep 3225, value loss 0.364, policy loss -0.018, reward -0.076\n",
      "\tstep 3348, value loss 0.358, policy loss -0.011, reward -0.080\n",
      "\tstep 3452, value loss 0.362, policy loss -0.012, reward -0.078\n",
      "\tstep 3544, value loss 0.358, policy loss -0.014, reward -0.070\n",
      "\tstep 3633, value loss 0.354, policy loss -0.015, reward -0.079\n",
      "\t\tsaved model_step_3723_reward_-0.077_.tar\n",
      "\tstep 3738, value loss 0.355, policy loss -0.015, reward -0.077\n",
      "\tstep 3851, value loss 0.365, policy loss -0.010, reward -0.078\n",
      "\tstep 3974, value loss 0.356, policy loss -0.013, reward -0.072\n",
      "\tstep 4102, value loss 0.362, policy loss -0.013, reward -0.072\n",
      "\tstep 4221, value loss 0.360, policy loss -0.015, reward -0.071\n",
      "\tstep 4345, value loss 0.356, policy loss -0.013, reward -0.070\n",
      "\t\tsaved model_step_4449_reward_-0.076_.tar\n",
      "\tstep 4468, value loss 0.353, policy loss -0.013, reward -0.078\n",
      "\tstep 4574, value loss 0.348, policy loss -0.009, reward -0.075\n",
      "\tstep 4668, value loss 0.353, policy loss -0.010, reward -0.076\n",
      "\tstep 4765, value loss 0.356, policy loss -0.010, reward -0.073\n",
      "\tstep 4873, value loss 0.356, policy loss -0.013, reward -0.067\n",
      "\tstep 4990, value loss 0.367, policy loss -0.012, reward -0.062\n",
      "\t\tsaved model_step_5090_reward_-0.065_.tar\n",
      "\tstep 5113, value loss 0.368, policy loss -0.011, reward -0.063\n",
      "\tstep 5224, value loss 0.370, policy loss -0.016, reward -0.068\n",
      "\tstep 5315, value loss 0.376, policy loss -0.014, reward -0.077\n",
      "\tstep 5409, value loss 0.371, policy loss -0.009, reward -0.074\n",
      "\tstep 5520, value loss 0.360, policy loss -0.011, reward -0.076\n",
      "\tstep 5634, value loss 0.362, policy loss -0.012, reward -0.078\n",
      "\t\tlol your model sucks keep training\n",
      "\tstep 5756, value loss 0.372, policy loss -0.015, reward -0.080\n",
      "\tstep 5875, value loss 0.370, policy loss -0.014, reward -0.079\n",
      "\tstep 5993, value loss 0.371, policy loss -0.013, reward -0.074\n",
      "\tstep 6104, value loss 0.361, policy loss -0.012, reward -0.075\n",
      "\tstep 6213, value loss 0.364, policy loss -0.010, reward -0.078\n",
      "\tstep 6325, value loss 0.358, policy loss -0.008, reward -0.081\n",
      "\t\tlol your model sucks keep training\n",
      "\tstep 6451, value loss 0.360, policy loss -0.008, reward -0.082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-24232f1eec4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoiselevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sam/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-feb0fd693f1c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sam/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sam/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sam/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NNPolicy(num_actions=args.num_actions)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=args.lr, momentum=0.9)\n",
    "run_vloss, run_ploss, run_reward = None, None, None\n",
    "\n",
    "print('training a simple policy gradient CNN agent on the towers RTS game')\n",
    "print_t = save_t = time.time()\n",
    "for step in range(args.total_steps + 1):\n",
    "    envs = [Towers() for i in range(args.batch_size)]\n",
    "    obs = np.stack([e.reset() for e in envs])\n",
    "    state = Variable(torch.Tensor(obs + np.random.randn(*obs.shape)*args.noiselevel))\n",
    "    value, probs = model(state)\n",
    "    \n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    actions = m.sample()\n",
    "    raw_reward = [e.step(actions.data.numpy().ravel()[i])[1] for i, e in enumerate(envs)]\n",
    "    raw_reward = Variable(torch.Tensor(raw_reward))\n",
    "    norm_reward = (raw_reward - raw_reward.mean())/raw_reward.std() # this sometimes helps convergence\n",
    "\n",
    "    vloss = 0.5*(value.view(-1) - norm_reward).pow(2).mean()\n",
    "    ploss = (-m.log_prob(actions) * norm_reward).mean()\n",
    "    (vloss + ploss).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    np_vloss = vloss.data.view(-1)[0]\n",
    "    np_ploss = ploss.data.view(-1)[0]\n",
    "    np_reward = raw_reward.mean().data.view(-1)[0]\n",
    "\n",
    "    run_vloss = np_vloss if run_vloss is None else 0.99*run_vloss + 0.01*np_vloss\n",
    "    run_ploss = np_ploss if run_ploss is None else 0.99*run_ploss + 0.01*np_ploss\n",
    "    run_reward = np_reward if run_reward is None else 0.99*run_reward + 0.01*np_reward\n",
    "    \n",
    "    if time.time() - print_t > args.printevery:\n",
    "        print_t = time.time()\n",
    "        print('\\tstep {}, value loss {:.3f}, policy loss {:.3f}, reward {:.3f}'\n",
    "              .format(step, run_vloss, run_ploss, run_reward))\n",
    "    \n",
    "    if time.time() - save_t > args.saveevery:\n",
    "        save_t = time.time()\n",
    "        paths = glob.glob(args.save_dir + '*.tar') ; rew_best_saved_model = -10000\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [float(s.split('_')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; rew_best_saved_model = ckpts[ix]\n",
    "        if rew_best_saved_model < run_reward:\n",
    "            name = 'model_step_{:.0f}_reward_{:.3f}_.tar'.format(step,run_reward)\n",
    "            print('\\t\\tsaved {}'.format(name))\n",
    "            torch.save(model.state_dict(), args.save_dir + name)\n",
    "        else:\n",
    "            print('\\t\\tlol your model sucks keep training')\n",
    "        \n",
    "    if run_reward > args.rewardthresh:\n",
    "        print(\"environment solved\")\n",
    "        name = 'model_step_{:.0f}_reward_{:.2f}_.tar'.format(step,run_reward) ; print('\\t\\tsaved {}'.format(name))\n",
    "        torch.save(model.state_dict(), args.save_dir + name)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try loading a saved model\n",
    "\n",
    "Yeah, the saving and loading I/O works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: ./saved/model_step_5090_reward_-0.065_.tar\n",
      "this saved model had a mean reward of -0.07\n"
     ]
    }
   ],
   "source": [
    "model = NNPolicy(num_actions=args.num_actions)\n",
    "ckpt_reward = model.try_load(args.save_dir)\n",
    "print('this saved model had a mean reward of {:.2f}'.format(ckpt_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
